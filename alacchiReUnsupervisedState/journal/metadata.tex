% DO NOT EDIT - automatically generated from metadata.yaml

\def \codeURL{https://github.com/rescience-c/template}
\def \codeDOI{}
\def \dataURL{}
\def \dataDOI{}
\def \editorNAME{Koustuv Sinha}
\def \editorORCID{0000-0002-2803-9236}
\def \reviewerINAME{}
\def \reviewerIORCID{}
\def \reviewerIINAME{}
\def \reviewerIIORCID{}
\def \dateRECEIVED{}
\def \dateACCEPTED{}
\def \datePUBLISHED{}
\def \articleTITLE{[Re] Unsupervised Representation Learning in Atari}
\def \articleTYPE{Replication}
\def \articleDOMAIN{NeurIPS 2019 Reproducibility Challenge}
\def \articleBIBLIOGRAPHY{bibliography.bib}
\def \articleYEAR{2020}
\def \reviewURL{}
\def \articleABSTRACT{In this study, we performed some ablations on the main model developed in the paper "Unsupervised Representation Learning in Atari" as part of the 2019 NeurIPS Reproducibility Challenge. In this paper, Anand et. al introduce a new learning method called SpatioTemporal DeepInfoMax (STDIM), which is an unsupervised method that aims at learning state representations by maximizing particular forms of mutual information between a series of observations. Our work focuses on recreating a subset of their results, along with hyperparameter tuning, slightly altering the STDIM learning objective, and altering the receptive ﬁeld of the encoder model that Anand et. al introduce in their article. We also suggest directions for further expanding the STDIM method. Our results also suggest that creating an ensemble model would allow for further boosting of the eﬀectiveness of this model.}
\def \replicationCITE{State representation learning, or the ability to capture latent generative factors of an environment, is crucial for building intelligent agents that can perform a wide variety of tasks. Learning such representations without supervision from rewards is a challenging open problem. We introduce a method that learns state representations by maximizing mutual information across spatially and temporally distinct features of a neural encoder of the observations. We also introduce a new benchmark based on Atari 2600 games where we evaluate representations based on how well they capture the ground truth state variables. We believe this new framework for evaluating representation learning models will be crucial for future representation learning research. Finally, we compare our technique with other state-of-the-art generative and contrastive representation learning methods. The code associated with this work is available at https://github.com/mila-iqia/atari-representation-learning}
\def \replicationBIB{}
\def \replicationURL{https://arxiv.org/pdf/1906.08226.pdf}
\def \replicationDOI{}
\def \contactNAME{Gabriel Alacchi}
\def \contactEMAIL{gabriel.alacchi@mail.mcgill.ca}
\def \articleKEYWORDS{rescience c, rescience x}
\def \journalNAME{ReScience C}
\def \journalVOLUME{6}
\def \journalISSUE{1}
\def \articleNUMBER{}
\def \articleDOI{}
\def \authorsFULL{Gabriel Alacchi, Guillaume Lam and Carl Perreault-Lafleur}
\def \authorsABBRV{G. Alacchi, G. Lam and C. Perreault-Lafleur}
\def \authorsSHORT{Alacchi, Lam and Perreault-Lafleur}
\title{\articleTITLE}
\date{}
\author[1,$\dagger$]{Gabriel Alacchi}
\author[1,$\dagger$]{Guillaume Lam}
\author[1,$\dagger$]{Carl Perreault-Lafleur}
\affil[1]{School of Computer Science, McGill University, Montreal, Canada}
\affil[$\dagger$]{Equal contributions}
